# Credit Score Classification

Проект по классификации кредитного скоринга: от загрузки датасета и очистки данных до обучения и сравнения моделей. Основная цель — построить воспроизводимый пайплайн подготовки данных и получить базовую метрику качества для моделей классификации.

## Обзор

Пайплайн проекта:
1. Загрузка датасета с Kaggle.
2. Очистка и приведение данных к корректным типам.
3. Сохранение обработанного набора в `data/processed/`.
4. EDA и моделирование в ноутбуках.

## Архитектура и поток данных

- **Слой данных**: сырые данные скачиваются в `data/raw/` и не изменяются.
- **Слой обработки**: скрипт `src/preprocessing.py` очищает данные и сохраняет итоговый CSV в `data/processed/`.
- **Слой анализа и моделирования**: ноутбуки в `notebooks/` используют обработанный CSV для EDA и обучения.

Архитектура организована по принципу разделения ответственности: каждый этап (загрузка, предобработка, анализ/модели) живет в отдельном модуле/ноутбуке.

## Технологии и обоснование выбора

- **Python** — основной язык, хорошо подходит для анализа данных и ML.
- **pandas, numpy** — стандарт де-факто для табличных данных и численных операций.
- **kaggle (Kaggle API)** — автоматизация получения датасета и воспроизводимость источника данных.
- **python-dotenv** — загрузка переменных окружения через `.env` для удобной настройки локального окружения.
- **scikit-learn** — набор базовых алгоритмов и утилит (`LabelEncoder`, `MinMaxScaler`, `train_test_split`, `cross_validate`) для воспроизводимого ML.
- **CatBoost** — современная модель градиентного бустинга, часто сильная на табличных данных.
- **matplotlib, seaborn** — визуализации и EDA.
- **Jupyter Notebook** — интерактивная разработка и анализ результатов.

## Паттерны и ключевые решения

- **Конфигурация очистки через `STEP_CONFIGS`**: список шагов задается в `src/preprocessing_config.py`, а функции очистки регистрируются через `register_step`. Это снижает связанность и позволяет добавлять новые шаги без переписывания пайплайна.
- **Функциональная регистрация шагов**: декоратор `@preprocessing_step` и реестр `PREPROCESSING_STEPS` формируют последовательность обработки данных в одном месте.
- **Групповая импутация модой**: для категориальных и числовых признаков используется восстановление пропусков по моде внутри групп, чтобы сохранить структуру данных внутри похожих пользователей.
- **Очистка аномалий по диапазонам**: выбросы заменяются на `NaN` с последующим заполнением модой/медианой — это стабилизирует распределения и уменьшает шум.
- **Stratified split и фиксированный `RANDOM_STATE`**: обеспечивает воспроизводимость и баланс классов в выборках.
- **Cross-validation**: метрики считаются на 5 фолдах, чтобы оценка качества была устойчивой.

## Структура проекта

```
Credit-Score-Classification/
├── data/                      # Данные (создается после скачивания)
│   ├── raw/                   # Сырые данные
│   └── processed/             # Обработанные данные
├── notebooks/                 # Jupyter ноутбуки
│   ├── eda.ipynb              # Исследовательский анализ данных
│   └── modeling.ipynb         # Моделирование и оценка
├── src/                       # Код пайплайна
│   ├── download_dataset.py    # Загрузка датасета
│   ├── preprocessing.py       # Очистка и подготовка данных
│   └── preprocessing_config.py# Конфигурация шагов очистки
├── .env.example               # Шаблон для переменных окружения
├── requirements.txt           # Зависимости
├── LICENSE
└── README.md
```

## Датасет

Источник: Kaggle — `parisrohan/credit-score-classification`.

Примеры признаков:
- Идентификаторы: `ID`, `Customer_ID`.
- Демография: `Age`, `Occupation`.
- Финансовые показатели: `Annual_Income`, `Monthly_Inhand_Salary`, `Outstanding_Debt`.
- Кредитная история: `Num_of_Loan`, `Credit_Utilization_Ratio`, `Delay_from_due_date`.
- Целевая переменная: `Credit_Score`.

## Подготовка окружения

1. Создать виртуальное окружение (рекомендуется):
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   ```

2. Установить зависимости:
   ```bash
   pip install -r requirements.txt
   ```

3. Для ноутбуков и моделирования дополнительно нужны:
   - `scikit-learn`
   - `seaborn`
   - `catboost`

## Загрузка данных

Скрипт использует Kaggle API. Убедитесь, что `kaggle.json` доступен в `~/.kaggle/` или переменные окружения настроены через `.env`:

```bash
python3 src/download_dataset.py
```

Сырые данные появятся в `data/raw/`.

## Предобработка данных

```bash
python3 src/preprocessing.py
```

Результат: `data/processed/train_processed.csv`.

## Моделирование

Основной ML-пайплайн реализован в ноутбуке:

```bash
jupyter notebook notebooks/modeling.ipynb
```

В ноутбуке:
- удаляются нерелевантные признаки (`ID`, `Customer_ID`, `Month`, `Name`, `SSN`);
- категориальные признаки кодируются (`LabelEncoder`);
- числовые признаки нормализуются (`MinMaxScaler`);
- сравниваются модели `DecisionTree`, `RandomForest`, `CatBoost` через `cross_validate`.

## EDA

```bash
jupyter notebook notebooks/eda.ipynb
```

EDA показывает распределения до/после очистки и подтверждает эффективность предобработки.

## Лицензия

MIT License, подробнее в `LICENSE`.
